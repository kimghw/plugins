#!/usr/bin/env python3
"""
PDF-Markdown ê²€ì¦ ìŠ¤í¬ë¦½íŠ¸

PDFì—ì„œ ì¶”ì¶œí•œ í…ìŠ¤íŠ¸ ë­‰ì¹˜(ë¬¸ì¥/ë‹¨ì–´)ê°€ ë§ˆí¬ë‹¤ìš´ì— í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.
ëˆ„ë½ëœ í•­ëª©ë§Œ ë¦¬í¬íŠ¸í•˜ì—¬ Claudeë¡œ ì¬ê²€í† í•  ìˆ˜ ìˆê²Œ í•©ë‹ˆë‹¤.
"""

import fitz  # PyMuPDF
import re
import sys
import argparse
import json
from pathlib import Path
from dataclasses import dataclass, field
from typing import List, Set


@dataclass
class MissingItem:
    """ëˆ„ë½ëœ í•­ëª©"""
    text: str
    page_num: int
    item_type: str  # 'sentence', 'phrase', 'keyword'


@dataclass
class VerificationReport:
    """ê²€ì¦ ê²°ê³¼ ë¦¬í¬íŠ¸"""
    pdf_file: str
    md_file: str
    total_chunks: int = 0
    found_chunks: int = 0
    missing_items: List[MissingItem] = field(default_factory=list)

    @property
    def coverage_rate(self) -> float:
        if self.total_chunks == 0:
            return 100.0
        return (self.found_chunks / self.total_chunks) * 100

    def to_dict(self) -> dict:
        return {
            'pdf_file': self.pdf_file,
            'md_file': self.md_file,
            'total_chunks': self.total_chunks,
            'found_chunks': self.found_chunks,
            'coverage_rate': round(self.coverage_rate, 2),
            'missing_count': len(self.missing_items),
            'missing_items': [
                {
                    'text': m.text,
                    'page': m.page_num,
                    'type': m.item_type
                }
                for m in self.missing_items
            ]
        }


class MarkdownVerifier:
    """ë§ˆí¬ë‹¤ìš´ ê²€ì¦ í´ë˜ìŠ¤ - ë‹¨ì–´/ë¬¸ì¥ í¬í•¨ ì—¬ë¶€ í™•ì¸"""

    # ë¬´ì‹œí•  íŒ¨í„´ (í˜ì´ì§€ ë²ˆí˜¸, ë¨¸ë¦¬ê¸€/ë°”ë‹¥ê¸€ ë“±)
    IGNORE_PATTERNS = [
        r'^- [ivx]+ -$',           # ë¡œë§ˆ ìˆ«ì í˜ì´ì§€ ë²ˆí˜¸
        r'^- \d+ -$',              # ìˆ«ì í˜ì´ì§€ ë²ˆí˜¸
        r'^\d+$',                   # ë‹¨ë… ìˆ«ì
        r'^ì„ ê¸‰ ë° ê°•ì„ ê·œì¹™ 2025$',  # ë¨¸ë¦¬ê¸€
        r'^ì„ ê¸‰ë°ê°•ì„ ê·œì¹™2025$',     # ë¨¸ë¦¬ê¸€ (ê³µë°± ì—†ìŒ)
        r'^1 í¸ \d+ ì¥$',           # ë¨¸ë¦¬ê¸€
        r'^\d+ í¸.+ê²€ì‚¬$',          # ë¨¸ë¦¬ê¸€ (1 í¸ì„ ê¸‰ë“±ë¡ë°ê²€ì‚¬)
        r'^\d+ í¸\d+ ì¥$',          # ë¨¸ë¦¬ê¸€ (1 í¸1 ì¥)
        r'^\d+ ì¥.+$',              # ë¨¸ë¦¬ê¸€ (1 ì¥ì„ ê¸‰ë“±ë¡)
        r'^[ivx]+$',                # ë¡œë§ˆ ìˆ«ìë§Œ
        r'^\.+$',                   # ì ë§Œ ìˆëŠ” ì¤„ (ëª©ì°¨)
        r'^Â·+$',                    # ê°€ìš´ë°ì 
        r'^RA-\d+-K$',              # ë¬¸ì„œë²ˆí˜¸
        r'^í•œ\s*êµ­\s*ì„ \s*ê¸‰$',      # í•œêµ­ì„ ê¸‰
        r'^\d+\s*í¸\s*ë¶€ë¡',          # ë¨¸ë¦¬ê¸€ (1 í¸ë¶€ë¡1-7)
        r'^ë¶€ë¡\d+-\d+',              # ë¨¸ë¦¬ê¸€ (ë¶€ë¡1-12-2 ...)
    ]

    def __init__(self, base_dir: str):
        self.base_dir = Path(base_dir)
        self.pdf_dir = self.base_dir / "ë¶„í• "
        self.md_dir = self.base_dir / "ë§ˆí¬ë‹¤ìš´"

    def extract_pdf_chunks(self, pdf_path: Path) -> List[tuple]:
        """
        PDFì—ì„œ ì˜ë¯¸ìˆëŠ” í…ìŠ¤íŠ¸ ë­‰ì¹˜ ì¶”ì¶œ

        Returns:
            List of (text, page_num, chunk_type) tuples
        """
        chunks = []
        doc = fitz.open(pdf_path)

        for page_num in range(len(doc)):
            page = doc[page_num]
            text = page.get_text("text")

            # ì¤„ ë‹¨ìœ„ë¡œ ë¶„ë¦¬
            lines = text.split('\n')

            for line in lines:
                line = line.strip()

                # ë¹ˆ ì¤„ ë¬´ì‹œ
                if not line:
                    continue

                # ë¬´ì‹œí•  íŒ¨í„´ ì²´í¬
                if self._should_ignore(line):
                    continue

                # ë„ˆë¬´ ì§§ì€ í…ìŠ¤íŠ¸ (1-2ì)ëŠ” í‚¤ì›Œë“œë¡œ ë¶„ë¥˜
                if len(line) <= 2:
                    # ë‹¨ë… ê¸€ìëŠ” ë¬´ì‹œ (ë ˆì´ì•„ì›ƒ ë¬¸ì œ)
                    continue

                # ë¬¸ì¥ì¸ì§€ êµ¬ë¬¸ì¸ì§€ íŒë‹¨
                if len(line) > 20:
                    chunk_type = 'sentence'
                elif len(line) > 5:
                    chunk_type = 'phrase'
                else:
                    chunk_type = 'keyword'

                chunks.append((line, page_num + 1, chunk_type))

        doc.close()
        return chunks

    def _should_ignore(self, text: str) -> bool:
        """ë¬´ì‹œí•´ì•¼ í•  í…ìŠ¤íŠ¸ì¸ì§€ í™•ì¸"""
        for pattern in self.IGNORE_PATTERNS:
            if re.match(pattern, text):
                return True
        return False

    def normalize_text(self, text: str) -> str:
        """í…ìŠ¤íŠ¸ ì •ê·œí™” (ë¹„êµìš©)"""
        # ê³µë°± ì •ê·œí™”
        text = re.sub(r'\s+', ' ', text)
        text = text.strip()

        # ì „ê° ë¬¸ìë¥¼ ë°˜ê°ìœ¼ë¡œ
        result = []
        for char in text:
            code = ord(char)
            if code == 0x3000:  # ì „ê° ê³µë°±
                result.append(' ')
            elif 0xFF01 <= code <= 0xFF5E:  # ì „ê° ASCII
                result.append(chr(code - 0xFEE0))
            else:
                result.append(char)

        return ''.join(result).lower()

    def extract_words(self, text: str) -> List[str]:
        """í…ìŠ¤íŠ¸ì—ì„œ ìˆœìˆ˜ ë‹¨ì–´ë§Œ ì¶”ì¶œ (íŠ¹ìˆ˜ë¬¸ì ì œê±°, 1ê¸€ì ì œì™¸)"""
        # í•œê¸€, ì˜ë¬¸, ìˆ«ìë§Œ ë‹¨ì–´ë¡œ ì¸ì‹, 2ê¸€ì ì´ìƒë§Œ
        return [w for w in re.findall(r'[ê°€-í£a-zA-Z0-9]+', text.lower()) if len(w) >= 2]

    def make_trigrams(self, words: List[str]) -> List[str]:
        """ì—°ì† 3ë‹¨ì–´ ì¡°í•©(trigram) ìƒì„±"""
        if len(words) < 3:
            return []
        return [f"{words[i]} {words[i+1]} {words[i+2]}" for i in range(len(words) - 2)]

    def read_markdown_text(self, md_path: Path) -> str:
        """ë§ˆí¬ë‹¤ìš´ íŒŒì¼ ì „ì²´ í…ìŠ¤íŠ¸ ì½ê¸° (ì •ê·œí™”)"""
        with open(md_path, 'r', encoding='utf-8') as f:
            text = f.read()

        # ë§ˆí¬ë‹¤ìš´ ë¬¸ë²• ì œê±°
        text = self._strip_markdown(text)

        return self.normalize_text(text)

    def _strip_markdown(self, text: str) -> str:
        """ë§ˆí¬ë‹¤ìš´ ë¬¸ë²• ì œê±°"""
        # ì´ë¯¸ì§€/ë§í¬
        text = re.sub(r'!\[([^\]]*)\]\([^)]+\)', r'\1', text)
        text = re.sub(r'\[([^\]]+)\]\([^)]+\)', r'\1', text)

        # êµµê²Œ/ê¸°ìš¸ì„
        text = re.sub(r'\*\*([^*]+)\*\*', r'\1', text)
        text = re.sub(r'\*([^*]+)\*', r'\1', text)
        text = re.sub(r'__([^_]+)__', r'\1', text)
        text = re.sub(r'_([^_]+)_', r'\1', text)

        # ì½”ë“œ
        text = re.sub(r'`([^`]+)`', r'\1', text)

        # ì œëª©/ëª©ë¡ ë§ˆí¬
        text = re.sub(r'^#{1,6}\s+', '', text, flags=re.MULTILINE)
        text = re.sub(r'^\s*[-*+]\s+', '', text, flags=re.MULTILINE)

        # í‘œ êµ¬ë¶„ì
        text = re.sub(r'\|', ' ', text)
        text = re.sub(r'^[-:]+$', '', text, flags=re.MULTILINE)

        # ìˆ˜í‰ì„ 
        text = re.sub(r'^---+$', '', text, flags=re.MULTILINE)

        # HTML ì£¼ì„
        text = re.sub(r'<!--.*?-->', '', text, flags=re.DOTALL)

        return text

    def verify(self, pdf_path: Path, md_path: Path) -> VerificationReport:
        """PDF í…ìŠ¤íŠ¸ê°€ ë§ˆí¬ë‹¤ìš´ì— í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸ (ì¤„ ë‹¨ìœ„ trigram ë°©ì‹)"""
        report = VerificationReport(
            pdf_file=str(pdf_path.name),
            md_file=str(md_path.name)
        )

        # ë§ˆí¬ë‹¤ìš´ ì „ì²´ í…ìŠ¤íŠ¸ì—ì„œ ë‹¨ì–´ë§Œ ì¶”ì¶œ â†’ trigram set ìƒì„±
        md_text_raw = self.read_markdown_text(md_path)
        md_words = self.extract_words(md_text_raw)
        md_trigrams = set(self.make_trigrams(md_words))

        # PDF í…ìŠ¤íŠ¸ ë­‰ì¹˜ ì¶”ì¶œ (ì¤„ ë‹¨ìœ„, IGNORE íŒ¨í„´ ì ìš©)
        pdf_chunks = self.extract_pdf_chunks(pdf_path)

        # ê° ì¤„ì—ì„œ ë‹¨ì–´ë§Œ ì¶”ì¶œ â†’ trigram ìƒì„±í•˜ì—¬ ë¹„êµ
        for chunk_text, page_num, chunk_type in pdf_chunks:
            words = self.extract_words(chunk_text)

            # ìˆœìˆ˜ ë‹¨ì–´ 3ê°œ ë¯¸ë§Œì´ë©´ ìŠ¤í‚µ
            if len(words) < 3:
                continue

            trigrams = self.make_trigrams(words)
            if not trigrams:
                continue

            report.total_chunks += 1

            # trigram ì¤‘ í•˜ë‚˜ë¼ë„ ë§ˆí¬ë‹¤ìš´ì— ìˆìœ¼ë©´ í¬í•¨ëœ ê²ƒìœ¼ë¡œ íŒì •
            found = any(t in md_trigrams for t in trigrams)

            if found:
                report.found_chunks += 1
            else:
                report.missing_items.append(MissingItem(
                    text=chunk_text,
                    page_num=page_num,
                    item_type=chunk_type
                ))

        return report

    def print_report(self, report: VerificationReport, verbose: bool = False):
        """ê²€ì¦ ë¦¬í¬íŠ¸ ì¶œë ¥"""
        print(f"\n{'='*60}")
        print(f"ê²€ì¦ ê²°ê³¼: {report.pdf_file}")
        print(f"{'='*60}")
        print(f"ì´ í…ìŠ¤íŠ¸ ë­‰ì¹˜: {report.total_chunks}ê°œ")
        print(f"í¬í•¨ í™•ì¸: {report.found_chunks}ê°œ")
        print(f"ì»¤ë²„ë¦¬ì§€: {report.coverage_rate:.1f}%")
        print(f"ëˆ„ë½ ì˜ì‹¬: {len(report.missing_items)}ê°œ")

        if verbose and report.missing_items:
            print(f"\n[ëˆ„ë½ ì˜ì‹¬ í•­ëª©] - Claudeë¡œ ì¬í™•ì¸ í•„ìš”")

            # í˜ì´ì§€ë³„ë¡œ ê·¸ë£¹í™”
            by_page = {}
            for item in report.missing_items:
                if item.page_num not in by_page:
                    by_page[item.page_num] = []
                by_page[item.page_num].append(item)

            for page_num in sorted(by_page.keys()):
                print(f"\n  ğŸ“„ í˜ì´ì§€ {page_num}:")
                for item in by_page[page_num][:10]:  # í˜ì´ì§€ë‹¹ ìµœëŒ€ 10ê°œ
                    print(f"    - [{item.item_type}] {item.text[:60]}{'...' if len(item.text) > 60 else ''}")
                if len(by_page[page_num]) > 10:
                    print(f"    ... ì™¸ {len(by_page[page_num]) - 10}ê°œ")

    def verify_single(self, pdf_path: Path, md_path: Path,
                      verbose: bool = True) -> VerificationReport:
        """ë‹¨ì¼ íŒŒì¼ ìŒ ê²€ì¦"""
        if not pdf_path.exists():
            print(f"Error: PDF íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {pdf_path}")
            return None

        if not md_path.exists():
            print(f"Error: ë§ˆí¬ë‹¤ìš´ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {md_path}")
            return None

        report = self.verify(pdf_path, md_path)
        self.print_report(report, verbose)
        return report

    def verify_all(self, verbose: bool = False) -> List[VerificationReport]:
        """ëª¨ë“  íŒŒì¼ ê²€ì¦"""
        reports = []
        md_files = sorted(self.md_dir.glob("*.md"))

        if not md_files:
            print(f"Error: ë§ˆí¬ë‹¤ìš´ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {self.md_dir}")
            return reports

        print(f"ì´ {len(md_files)}ê°œ ë§ˆí¬ë‹¤ìš´ íŒŒì¼ ê²€ì¦ ì‹œì‘\n")

        for i, md_path in enumerate(md_files, 1):
            pdf_path = self.pdf_dir / f"{md_path.stem}.pdf"

            if not pdf_path.exists():
                print(f"[{i}/{len(md_files)}] {md_path.name}: PDF ì—†ìŒ")
                continue

            print(f"[{i}/{len(md_files)}] ê²€ì¦ ì¤‘: {md_path.name}")
            report = self.verify(pdf_path, md_path)
            reports.append(report)

            status = "OK" if report.coverage_rate >= 90 else "í™•ì¸í•„ìš”"
            print(f"  â†’ ì»¤ë²„ë¦¬ì§€: {report.coverage_rate:.1f}%, ëˆ„ë½ì˜ì‹¬: {len(report.missing_items)}ê°œ [{status}]")

        # ìš”ì•½
        print(f"\n{'='*60}")
        print("ì „ì²´ ê²€ì¦ ìš”ì•½")
        print(f"{'='*60}")
        total_files = len(reports)
        ok_files = len([r for r in reports if r.coverage_rate >= 90])
        print(f"ê²€ì¦ íŒŒì¼: {total_files}ê°œ")
        print(f"ì–‘í˜¸ (90%+): {ok_files}ê°œ")
        print(f"í™•ì¸ í•„ìš”: {total_files - ok_files}ê°œ")

        return reports

    def export_report(self, reports: List[VerificationReport], output_path: Path):
        """ê²€ì¦ ê²°ê³¼ë¥¼ JSONìœ¼ë¡œ ì €ì¥"""
        data = {
            'summary': {
                'total_files': len(reports),
                'ok_files': len([r for r in reports if r.coverage_rate >= 90]),
                'avg_coverage': sum(r.coverage_rate for r in reports) / len(reports) if reports else 0
            },
            'reports': [r.to_dict() for r in reports]
        }

        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)

        print(f"\nê²€ì¦ ê²°ê³¼ ì €ì¥: {output_path}")


def main():
    parser = argparse.ArgumentParser(
        description='PDF í…ìŠ¤íŠ¸ê°€ ë§ˆí¬ë‹¤ìš´ì— í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ ê²€ì¦í•©ë‹ˆë‹¤.',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ì˜ˆì‹œ:
  # ë‹¨ì¼ íŒŒì¼ ê²€ì¦
  python verify_markdown.py ë¶„í• /ê°•ì„ ê·œì¹™_0001-0010.pdf ë§ˆí¬ë‹¤ìš´/ê°•ì„ ê·œì¹™_0001-0010.md

  # ì „ì²´ íŒŒì¼ ê²€ì¦
  python verify_markdown.py --all

  # ìƒì„¸ ì¶œë ¥
  python verify_markdown.py --all -v

  # JSONìœ¼ë¡œ ê²°ê³¼ ì €ì¥
  python verify_markdown.py --all --export report.json
        """
    )

    parser.add_argument('pdf_path', nargs='?', help='ê²€ì¦í•  PDF íŒŒì¼ ê²½ë¡œ')
    parser.add_argument('md_path', nargs='?', help='ê²€ì¦í•  ë§ˆí¬ë‹¤ìš´ íŒŒì¼ ê²½ë¡œ')
    parser.add_argument('--all', action='store_true', help='ë§ˆí¬ë‹¤ìš´ ë””ë ‰í† ë¦¬ì˜ ëª¨ë“  íŒŒì¼ ê²€ì¦')
    parser.add_argument('--base-dir', default=None, help='ê¸°ë³¸ ë””ë ‰í† ë¦¬ ê²½ë¡œ')
    parser.add_argument('-v', '--verbose', action='store_true', help='ìƒì„¸ ì¶œë ¥')
    parser.add_argument('--export', metavar='FILE', help='ê²€ì¦ ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥')

    args = parser.parse_args()

    if args.base_dir:
        base_dir = Path(args.base_dir)
    else:
        base_dir = Path(__file__).parent.parent

    verifier = MarkdownVerifier(base_dir)

    if args.all:
        reports = verifier.verify_all(verbose=args.verbose)
        if args.export and reports:
            verifier.export_report(reports, Path(args.export))
    elif args.pdf_path and args.md_path:
        pdf_path = Path(args.pdf_path)
        md_path = Path(args.md_path)

        if not pdf_path.is_absolute():
            pdf_path = base_dir / pdf_path
        if not md_path.is_absolute():
            md_path = base_dir / md_path

        report = verifier.verify_single(pdf_path, md_path, verbose=args.verbose)

        if args.export and report:
            verifier.export_report([report], Path(args.export))
    else:
        parser.print_help()
        sys.exit(1)


if __name__ == '__main__':
    main()
